{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d8bf5a3",
   "metadata": {},
   "source": [
    "# Core ML Model Validation Notebook\n",
    "\n",
    "This notebook validates converted Core ML style transfer models by:\n",
    "1. Loading and inspecting model metadata\n",
    "2. Testing inference on sample images\n",
    "3. Measuring performance (FPS, latency)\n",
    "4. Comparing visual quality\n",
    "5. Generating validation reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4774de81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import json\n",
    "\n",
    "try:\n",
    "    import coremltools as ct\n",
    "    from coremltools.models import MLModel\n",
    "except ImportError:\n",
    "    print(\"Installing coremltools...\")\n",
    "    !pip install coremltools\n",
    "    import coremltools as ct\n",
    "    from coremltools.models import MLModel\n",
    "\n",
    "print(f\"Core ML Tools version: {ct.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8571c89",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8913af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model to validate\n",
    "MODEL_PATH = \"../models/exported/sci-fi.mlmodel\"  # Change this to your model\n",
    "TEST_IMAGE_PATH = \"../datasets/sample/content/test_001.jpg\"  # Test image\n",
    "\n",
    "# Performance testing\n",
    "NUM_WARMUP_RUNS = 5\n",
    "NUM_BENCHMARK_RUNS = 20\n",
    "\n",
    "# Output directory\n",
    "OUTPUT_DIR = Path(\"./validation_results\")\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f3ae70",
   "metadata": {},
   "source": [
    "## 1. Load and Inspect Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc12c1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inspect_model(model_path):\n",
    "    \"\"\"Inspect Core ML model metadata and structure.\"\"\"\n",
    "    print(f\"Loading model: {model_path}\\n\")\n",
    "    \n",
    "    model = MLModel(str(model_path))\n",
    "    spec = model.get_spec()\n",
    "    \n",
    "    # Model metadata\n",
    "    print(\"=\" * 60)\n",
    "    print(\"MODEL METADATA\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Author: {model.author}\")\n",
    "    print(f\"Description: {model.short_description}\")\n",
    "    print(f\"Version: {model.version}\")\n",
    "    \n",
    "    # Custom metadata\n",
    "    if spec.description.metadata.userDefined:\n",
    "        print(\"\\nCustom Metadata:\")\n",
    "        for key, value in spec.description.metadata.userDefined.items():\n",
    "            print(f\"  {key}: {value}\")\n",
    "    \n",
    "    # Input/Output specs\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"INPUT/OUTPUT SPECIFICATIONS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    for inp in spec.description.input:\n",
    "        print(f\"\\nInput: {inp.name}\")\n",
    "        print(f\"  Type: {inp.type.WhichOneof('Type')}\")\n",
    "        if inp.type.multiArrayType:\n",
    "            print(f\"  Shape: {list(inp.type.multiArrayType.shape)}\")\n",
    "            print(f\"  Data Type: {inp.type.multiArrayType.dataType}\")\n",
    "        print(f\"  Description: {inp.shortDescription}\")\n",
    "    \n",
    "    for out in spec.description.output:\n",
    "        print(f\"\\nOutput: {out.name}\")\n",
    "        print(f\"  Type: {out.type.WhichOneof('Type')}\")\n",
    "        if out.type.multiArrayType:\n",
    "            print(f\"  Shape: {list(out.type.multiArrayType.shape)}\")\n",
    "            print(f\"  Data Type: {out.type.multiArrayType.dataType}\")\n",
    "        print(f\"  Description: {out.shortDescription}\")\n",
    "    \n",
    "    return model, spec\n",
    "\n",
    "model, spec = inspect_model(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c422da",
   "metadata": {},
   "source": [
    "## 2. Load and Preprocess Test Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a55cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_image(image_path, target_size=(256, 256), framework='pytorch'):\n",
    "    \"\"\"Load and preprocess image for model input.\"\"\"\n",
    "    img = Image.open(image_path).convert('RGB')\n",
    "    img = img.resize(target_size)\n",
    "    \n",
    "    img_array = np.array(img).astype(np.float32) / 255.0\n",
    "    \n",
    "    if framework == 'pytorch':\n",
    "        # PyTorch: (B, C, H, W)\n",
    "        img_array = np.transpose(img_array, (2, 0, 1))\n",
    "        img_array = np.expand_dims(img_array, axis=0)\n",
    "    else:\n",
    "        # TensorFlow: (B, H, W, C)\n",
    "        img_array = np.expand_dims(img_array, axis=0)\n",
    "    \n",
    "    return img, img_array\n",
    "\n",
    "def display_images(original, styled, title=\"Style Transfer Result\"):\n",
    "    \"\"\"Display original and styled images side by side.\"\"\"\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "    \n",
    "    axes[0].imshow(original)\n",
    "    axes[0].set_title('Original')\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    axes[1].imshow(styled)\n",
    "    axes[1].set_title('Stylized')\n",
    "    axes[1].axis('off')\n",
    "    \n",
    "    plt.suptitle(title)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Detect framework from model\n",
    "framework = spec.description.metadata.userDefined.get('framework', 'pytorch')\n",
    "print(f\"Detected framework: {framework}\")\n",
    "\n",
    "# Get input shape\n",
    "input_shape = spec.description.input[0].type.multiArrayType.shape\n",
    "if framework == 'pytorch':\n",
    "    h, w = int(input_shape[2]), int(input_shape[3])\n",
    "else:\n",
    "    h, w = int(input_shape[1]), int(input_shape[2])\n",
    "\n",
    "print(f\"Input size: {h}x{w}\")\n",
    "\n",
    "# Load test image\n",
    "original_img, input_array = load_and_preprocess_image(\n",
    "    TEST_IMAGE_PATH,\n",
    "    target_size=(w, h),\n",
    "    framework=framework\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.imshow(original_img)\n",
    "plt.title('Original Test Image')\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Input array shape: {input_array.shape}\")\n",
    "print(f\"Input array range: [{input_array.min():.3f}, {input_array.max():.3f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddec1dbc",
   "metadata": {},
   "source": [
    "## 3. Run Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eebfdfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference(model, input_array):\n",
    "    \"\"\"Run model inference.\"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    output = model.predict({'input_image': input_array})\n",
    "    \n",
    "    inference_time = time.time() - start_time\n",
    "    \n",
    "    return output['stylized_image'], inference_time\n",
    "\n",
    "def postprocess_output(output_array, framework='pytorch'):\n",
    "    \"\"\"Convert model output to displayable image.\"\"\"\n",
    "    if framework == 'pytorch':\n",
    "        # (B, C, H, W) -> (H, W, C)\n",
    "        output_array = np.transpose(output_array[0], (1, 2, 0))\n",
    "    else:\n",
    "        # (B, H, W, C) -> (H, W, C)\n",
    "        output_array = output_array[0]\n",
    "    \n",
    "    # Clip and convert to uint8\n",
    "    output_array = np.clip(output_array * 255, 0, 255).astype(np.uint8)\n",
    "    \n",
    "    return Image.fromarray(output_array)\n",
    "\n",
    "# Run inference\n",
    "print(\"Running inference...\")\n",
    "output_array, inference_time = run_inference(model, input_array)\n",
    "\n",
    "print(f\"Inference time: {inference_time*1000:.2f} ms\")\n",
    "print(f\"Output shape: {output_array.shape}\")\n",
    "print(f\"Output range: [{output_array.min():.3f}, {output_array.max():.3f}]\")\n",
    "\n",
    "# Postprocess and display\n",
    "styled_img = postprocess_output(output_array, framework)\n",
    "display_images(original_img, styled_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b1c8b6",
   "metadata": {},
   "source": [
    "## 4. Performance Benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d85588",
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_model(model, input_array, num_warmup=5, num_runs=20):\n",
    "    \"\"\"Benchmark model performance.\"\"\"\n",
    "    print(f\"Warming up ({num_warmup} runs)...\")\n",
    "    for _ in range(num_warmup):\n",
    "        _ = model.predict({'input_image': input_array})\n",
    "    \n",
    "    print(f\"\\nBenchmarking ({num_runs} runs)...\")\n",
    "    times = []\n",
    "    for i in range(num_runs):\n",
    "        start_time = time.time()\n",
    "        _ = model.predict({'input_image': input_array})\n",
    "        elapsed = time.time() - start_time\n",
    "        times.append(elapsed)\n",
    "        \n",
    "        if (i + 1) % 5 == 0:\n",
    "            print(f\"  Progress: {i + 1}/{num_runs}\")\n",
    "    \n",
    "    times = np.array(times)\n",
    "    \n",
    "    results = {\n",
    "        'mean_ms': np.mean(times) * 1000,\n",
    "        'std_ms': np.std(times) * 1000,\n",
    "        'min_ms': np.min(times) * 1000,\n",
    "        'max_ms': np.max(times) * 1000,\n",
    "        'median_ms': np.median(times) * 1000,\n",
    "        'fps': 1.0 / np.mean(times)\n",
    "    }\n",
    "    \n",
    "    return results, times\n",
    "\n",
    "# Run benchmark\n",
    "results, times = benchmark_model(\n",
    "    model,\n",
    "    input_array,\n",
    "    num_warmup=NUM_WARMUP_RUNS,\n",
    "    num_runs=NUM_BENCHMARK_RUNS\n",
    ")\n",
    "\n",
    "# Display results\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"PERFORMANCE RESULTS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Mean inference time: {results['mean_ms']:.2f} Â± {results['std_ms']:.2f} ms\")\n",
    "print(f\"Median inference time: {results['median_ms']:.2f} ms\")\n",
    "print(f\"Min inference time: {results['min_ms']:.2f} ms\")\n",
    "print(f\"Max inference time: {results['max_ms']:.2f} ms\")\n",
    "print(f\"Average FPS: {results['fps']:.2f}\")\n",
    "\n",
    "# Plot timing distribution\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(times * 1000, bins=20, edgecolor='black')\n",
    "plt.xlabel('Inference Time (ms)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Inference Time Distribution')\n",
    "plt.axvline(results['mean_ms'], color='r', linestyle='--', label=f\"Mean: {results['mean_ms']:.2f} ms\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(times * 1000, marker='o', markersize=3)\n",
    "plt.xlabel('Run Number')\n",
    "plt.ylabel('Inference Time (ms)')\n",
    "plt.title('Inference Time Over Runs')\n",
    "plt.axhline(results['mean_ms'], color='r', linestyle='--', alpha=0.7)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / 'performance_benchmark.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75d76828",
   "metadata": {},
   "source": [
    "## 5. Quality Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7813b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assess_quality(original, styled):\n",
    "    \"\"\"Assess visual quality metrics.\"\"\"\n",
    "    original_np = np.array(original).astype(np.float32)\n",
    "    styled_np = np.array(styled).astype(np.float32)\n",
    "    \n",
    "    # Mean Squared Error\n",
    "    mse = np.mean((original_np - styled_np) ** 2)\n",
    "    \n",
    "    # Peak Signal-to-Noise Ratio\n",
    "    if mse == 0:\n",
    "        psnr = float('inf')\n",
    "    else:\n",
    "        psnr = 20 * np.log10(255.0 / np.sqrt(mse))\n",
    "    \n",
    "    # Color distribution\n",
    "    original_mean = original_np.mean(axis=(0, 1))\n",
    "    styled_mean = styled_np.mean(axis=(0, 1))\n",
    "    \n",
    "    original_std = original_np.std(axis=(0, 1))\n",
    "    styled_std = styled_np.std(axis=(0, 1))\n",
    "    \n",
    "    return {\n",
    "        'mse': mse,\n",
    "        'psnr': psnr,\n",
    "        'original_color_mean': original_mean.tolist(),\n",
    "        'styled_color_mean': styled_mean.tolist(),\n",
    "        'original_color_std': original_std.tolist(),\n",
    "        'styled_color_std': styled_std.tolist()\n",
    "    }\n",
    "\n",
    "quality_metrics = assess_quality(original_img, styled_img)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"QUALITY METRICS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Mean Squared Error: {quality_metrics['mse']:.2f}\")\n",
    "print(f\"PSNR: {quality_metrics['psnr']:.2f} dB\")\n",
    "print(f\"\\nOriginal Color Mean (RGB): {quality_metrics['original_color_mean']}\")\n",
    "print(f\"Styled Color Mean (RGB): {quality_metrics['styled_color_mean']}\")\n",
    "print(f\"\\nOriginal Color Std (RGB): {quality_metrics['original_color_std']}\")\n",
    "print(f\"Styled Color Std (RGB): {quality_metrics['styled_color_std']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09690515",
   "metadata": {},
   "source": [
    "## 6. Generate Validation Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d48ab23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile validation report\n",
    "validation_report = {\n",
    "    'model_path': str(MODEL_PATH),\n",
    "    'test_image': str(TEST_IMAGE_PATH),\n",
    "    'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'model_info': {\n",
    "        'author': model.author,\n",
    "        'description': model.short_description,\n",
    "        'version': model.version,\n",
    "        'framework': framework,\n",
    "        'input_shape': list(input_shape),\n",
    "    },\n",
    "    'performance': results,\n",
    "    'quality': quality_metrics,\n",
    "    'verdict': {\n",
    "        'realtime_capable': results['fps'] >= 15,\n",
    "        'mobile_optimized': results['mean_ms'] <= 100,\n",
    "        'quality_acceptable': quality_metrics['psnr'] >= 20\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save report\n",
    "report_path = OUTPUT_DIR / 'validation_report.json'\n",
    "with open(report_path, 'w') as f:\n",
    "    json.dump(validation_report, f, indent=2)\n",
    "\n",
    "print(f\"\\nâœ“ Validation report saved to: {report_path}\")\n",
    "\n",
    "# Save output image\n",
    "output_img_path = OUTPUT_DIR / 'styled_output.jpg'\n",
    "styled_img.save(output_img_path)\n",
    "print(f\"âœ“ Styled image saved to: {output_img_path}\")\n",
    "\n",
    "# Print verdict\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"VALIDATION VERDICT\")\n",
    "print(\"=\" * 60)\n",
    "for key, value in validation_report['verdict'].items():\n",
    "    status = \"âœ“ PASS\" if value else \"âœ— FAIL\"\n",
    "    print(f\"{key.replace('_', ' ').title()}: {status}\")\n",
    "\n",
    "print(\"\\nðŸŽ‰ Validation complete!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
